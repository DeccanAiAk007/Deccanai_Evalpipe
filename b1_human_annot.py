import torch
import re
import pandas as pd
import json
from tqdm import tqdm
from sklearn.metrics import mean_absolute_error
import requests
import logging

# Set up logging
logging.basicConfig(level=logging.INFO)

# Constants
# generator_model = "deepseek-coder:6.7b"
generator_model = "mistral"
verifier_model = "mistral"
metrics = [
    "Relevance", "Succinctness", "Medical Correctness",
    "Hallucination", "Completeness", "Coherence"
]

def build_generation_prompt(question):
    return f"""You are a highly knowledgeable and trustworthy medical assistant.

Answer ONLY the specific question asked below. Be **factually correct**, **medically accurate**, and **concise**.
Do NOT include disclaimers, definitions, or unrelated information.
Do NOT hallucinate or make assumptions beyond the question.

Question: {question}
Answer:"""

def build_verification_prompt(question, ground_truth, model_output):
    return f"""
You are a strict and detail-oriented medical evaluator assessing the quality of responses generated by a language model in a clinical context. Your task is to compare the **Model Answer** with the **Gold Answer** (ground truth) and assign a score from -1 to 1 for each of the six evaluation metrics based on the criteria outlined below.

### Evaluation Metrics and Scoring Guidelines

1. **Relevance** – Does the response directly address the question posed?
   - -1: Completely irrelevant
   -  0: Partially relevant
   -  1: Fully relevant

2. **Succinctness** – Is the response concise, avoiding unnecessary detail?
   - -1: Verbose or overly wordy
   -  0: Somewhat concise
   -  1: Highly succinct

3. **Medical Correctness** – Is the information clinically accurate?
   - -1: Contains clinically dangerous errors
   -  0: Minor or benign inaccuracies
   -  1: Fully medically accurate

4. **Hallucination** – Does the response avoid adding incorrect or fabricated information?
   - -1: Includes fabricated or unsupported claims
   -  0: Minor hallucinations
   -  1: No hallucinations

5. **Completeness** – Does the answer fully address all aspects of the question?
   - -1: Severely incomplete
   -  0: Moderately complete
   -  1: Fully complete

6. **Coherence** – Is the response clear, logically structured, and easy to follow?
   - -1: Disorganized or incoherent
   -  0: Somewhat coherent
   -  1: Highly coherent

### Output Format (STRICT)

Output **only** the following JSON object. Do not include any explanation or extra text.

--- Input ---
Question: {question}

Gold Answer: {ground_truth}

Model Answer: {model_output}
--- End ---

Output only this JSON:
{{
  "Relevance": 1,
  "Succinctness": 1,
  "Medical Correctness": 1,
  "Hallucination": 1,
  "Completeness": 1,
  "Coherence": 1
}}"
"""

def extract_json(text):
    """
    Extract the first valid JSON object containing all expected metric keys.
    """
    stack = []
    start = None
    for i, char in enumerate(text):
        if char == '{':
            if not stack:
                start = i
            stack.append('{')
        elif char == '}':
            if stack:
                stack.pop()
                if not stack and start is not None:
                    candidate = text[start:i+1]
                    try:
                        obj = json.loads(candidate)
                        if all(k in obj and isinstance(obj[k], int) for k in metrics):
                            return obj
                        else:
                            logging.warning(f"Missing or invalid metric keys in extracted JSON: {candidate}")
                    except json.JSONDecodeError as e:
                        logging.warning(f"JSON decoding failed: {e}")
    logging.warning(f"No valid JSON found in:\n{text}")
    return {}


# Query ollama
def query_ollama(prompt, model="llama3.2", expect_json=False, max_tokens=512):
    try:
        url = "http://localhost:11434/api/generate"  # using generate instead of chat
        payload = {
            "model": model,
            "prompt": prompt,
            "stream": False,
            "options": {
                "num_predict": max_tokens,
                "temperature": 0.7,
                "repetition_penalty": 1.1,
                "top_p": 0.9
            }
        }

        response = requests.post(url, json=payload)
        response.raise_for_status()
        result = response.json()
        output = result.get("response", "").strip()

        if not output:
            logging.error(f"No generation received from model {model} for prompt:\n{prompt}")
            return {} if expect_json else ""

        return extract_json(output) if expect_json else output

    except requests.RequestException as e:
        logging.error(f"Ollama API request failed: {e}")
        return {} if expect_json else ""


def evaluate_pipeline(input_csv, output_csv):
    df = pd.read_csv(input_csv)
    results = []

    for _, row in tqdm(df.iterrows(), total=len(df)):
        question = row["question"]
        gold = row["ground_truth"]
        row_data = row.to_dict()

        g_prompt = build_generation_prompt(question)
        gen_answer = query_ollama(g_prompt, model=generator_model)

        if not gen_answer:
            logging.error(f" Empty generation for question:\n{question}")
            for m in metrics:
                row_data[f"{m}_llm"] = -1
            row_data["model_output"] = ""
            results.append(row_data)
            continue

        v_prompt = build_verification_prompt(question, gold, gen_answer)
        scores = query_ollama(v_prompt, model=verifier_model, expect_json=True)
        
        for m in metrics:
            try:
                score = scores.get(m, -1)
                row_data[f"{m}_llm"] = int(score) if score is not None else -1
            except:
                row_data[f"{m}_llm"] = -1
                logging.warning(f"Invalid score for {m} on question: {question}")

        row_data["model_output"] = gen_answer
        results.append(row_data)

    pd.DataFrame(results).to_csv(output_csv, index=False)
    print(f"\n Results saved to {output_csv}")



def compute_mae(df):
    maes = {}
    for m in metrics:
        if f"{m}_human" in df.columns and f"{m}_llm" in df.columns:
            valid = df[df[f"{m}_llm"].between(-1,1)]
            if not valid.empty:
                maes[m] = mean_absolute_error(valid[f"{m}_human"], valid[f"{m}_llm"])
            else:
                maes[m] = "N/A"
    print("\n Mean Absolute Error (LLM vs Human):")
    for m, v in maes.items():
        print(f"{m}: {v}")

if __name__ == "__main__":
    input_csv = "med_bin.csv"
    output_csv = "zero_shot2.csv"
    evaluate_pipeline(input_csv, output_csv)
    df = pd.read_csv(output_csv)
    compute_mae(df)
